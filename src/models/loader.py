"""
æ¨¡å‹åŠ è½½æ¨¡å—
"""
import os
import torch
from ..config.settings import DEFAULT_CONFIG
from ..utils import log

# å…¨å±€æ¨¡å‹å’Œtokenizer
model = None
tokenizer = None
device = None

def load_model_interface(model_path):
    """åŠ è½½æ¨¡å‹ç•Œé¢å‡½æ•°"""
    global model, tokenizer, device
    
    if not model_path or model_path.strip() == "":
        model_path = DEFAULT_CONFIG["model_path"]
    
    if not os.path.exists(model_path):
        return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}", False
    
    try:
        log("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # åŠ¨æ€å¯¼å…¥
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        
        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        
        model.eval()
        
        info = f"""
âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼
æ¨¡å‹è·¯å¾„: {model_path}
ä½¿ç”¨è®¾å¤‡: {device}
æ¨¡å‹å‚æ•°é‡: çº¦0.5B
Tokenizer: å·²åŠ è½½
        """
        
        log(info)
        return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ", True
        
    except Exception as e:
        error_msg = f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}"
        log(error_msg)
        return error_msg, False

def get_model():
    """è·å–å½“å‰æ¨¡å‹"""
    return model, tokenizer, device

def is_model_loaded():
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
    return model is not None and tokenizer is not None
