"""
æ¨¡å‹è¯„ä¼°æ¨¡å—
"""
import os
import json
import threading
import time
import tempfile
import subprocess
import re
import traceback
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from ..config.settings import DEFAULT_CONFIG
from ..utils import log

comparison_results = {}

class EvaluationThread(threading.Thread):
    """è¯„ä¼°çº¿ç¨‹"""
    def __init__(self, config, callback=None):
        super().__init__()
        self.config = config
        self.callback = callback
        self.daemon = True
        self.result = None
        
    def log(self, message):
        if self.callback:
            self.callback(message)
        log(message)
        
    def run(self):
        try:
            self.evaluate_models()
        except Exception as e:
            self.log(f"è¯„ä¼°çº¿ç¨‹å‡ºé”™: {str(e)}")
            self.log(traceback.format_exc())
        finally:
            global is_evaluating
            is_evaluating = False
            
    def evaluate_models(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.log("å¼€å§‹å¯¼å…¥è¯„ä¼°åº“...")
        
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        original_path = self.config["model_path"]
        finetuned_path = self.config["finetuned_model_path"]
        dataset_path = self.config["human_eval_path"]
        
        if not os.path.exists(original_path):
            self.log(f" åŸå§‹æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {original_path}")
            return
            
        if not os.path.exists(finetuned_path):
            self.log(f" å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {finetuned_path}")
            return
            
        if not os.path.exists(dataset_path):
            self.log(f" HumanEvalæ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
            self.log("è¯·ä» https://github.com/openai/human-eval ä¸‹è½½æ•°æ®é›†")
            return
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹
        self.log("="*60)
        self.log("å¼€å§‹è¯„ä¼°åŸå§‹æ¨¡å‹...")
        original_result = self.evaluate_single_model(
            original_path, 
            "åŸå§‹æ¨¡å‹",
            base_model_path=None
        )
        
        if original_result:
            self.log(f"åŸå§‹æ¨¡å‹è¯„ä¼°å®Œæˆ: é€šè¿‡ç‡ {original_result['pass_rate']:.2f}%")
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        # è¯„ä¼°å¾®è°ƒæ¨¡å‹
        self.log("="*60)
        self.log("å¼€å§‹è¯„ä¼°å¾®è°ƒåæ¨¡å‹...")
        finetuned_result = self.evaluate_single_model(
            finetuned_path,
            "å¾®è°ƒåæ¨¡å‹",
            base_model_path=original_path  # LoRAéœ€è¦åŸºç¡€æ¨¡å‹
        )
        
        if finetuned_result:
            self.log(f"å¾®è°ƒåæ¨¡å‹è¯„ä¼°å®Œæˆ: é€šè¿‡ç‡ {finetuned_result['pass_rate']:.2f}%")
            
        # å¯¹æ¯”ç»“æœ
        if original_result and finetuned_result:
            comparison = self.compare_results(original_result, finetuned_result)
            global comparison_results
            comparison_results = comparison
            
            self.log("="*60)
            self.log("æ¨¡å‹å¯¹æ¯”å®Œæˆï¼")
            self.log(f"åŸå§‹æ¨¡å‹é€šè¿‡ç‡: {original_result['pass_rate']:.2f}%")
            self.log(f"å¾®è°ƒåæ¨¡å‹é€šè¿‡ç‡: {finetuned_result['pass_rate']:.2f}%")
            self.log(f"æå‡: {comparison['improvement']:.2f}%")
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_file = f"./evaluation_results_{timestamp}.json"
            
            results = {
                "original": original_result,
                "finetuned": finetuned_result,
                "comparison": comparison,
                "timestamp": timestamp,
                "config": self.config
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
                
            self.log(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            
        self.log("ğŸ‰ æ¨¡å‹è¯„ä¼°å…¨éƒ¨å®Œæˆï¼")
        
    def evaluate_single_model(self, model_path, model_name, base_model_path=None):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            # åŠ è½½æ¨¡å‹
            self.log(f"åŠ è½½{model_name}: {model_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯LoRA adapter
            is_lora = os.path.exists(os.path.join(model_path, "adapter_config.json"))
            
            if is_lora and base_model_path:
                # ä½¿ç”¨LoRA adapter
                self.log("æ£€æµ‹åˆ°LoRA adapterï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹å¹¶åˆå¹¶adapter")
                
                try:
                    from peft import PeftModel
                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹
                    tokenizer = AutoTokenizer.from_pretrained(
                        base_model_path,
                        local_files_only=True,
                        trust_remote_code=True
                    )
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        base_model_path,
                        local_files_only=True,
                        device_map="auto",
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        low_cpu_mem_usage=True,
                        trust_remote_code=True
                    )
                    
                    # åŠ è½½LoRA adapter
                    model = PeftModel.from_pretrained(model, model_path)
                    
                except ImportError:
                    self.log(" æœªå®‰è£…peftåº“ï¼Œæ— æ³•åŠ è½½LoRA adapter")
                    return None
                    
            else:
                # åŠ è½½å®Œæ•´æ¨¡å‹
                tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    local_files_only=True,
                    trust_remote_code=True
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    local_files_only=True,
                    device_map="auto",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
            
            # ç¡®ä¿tokenizerè®¾ç½®æ­£ç¡®
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            model.eval()
            
            # è¯»å–HumanEvalæ•°æ®é›†
            self.log("è¯»å–HumanEvalæ•°æ®é›†...")
            tasks = []
            with open(self.config["human_eval_path"], 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        tasks.append(json.loads(line))
            
            max_tasks = self.config.get("max_tasks", None)
            if max_tasks:
                tasks = tasks[:max_tasks]
                self.log(f"é™åˆ¶è¯„ä¼°ä»»åŠ¡æ•°: {max_tasks}")
            
            total_tasks = len(tasks)
            passed_tasks = 0
            failed_tasks = []
            detailed_results = []
            
            self.log(f"å¼€å§‹è¯„ä¼° {total_tasks} ä¸ªä»»åŠ¡...")
            start_time = time.time()
            
            for idx, task in enumerate(tasks, 1):
                task_id = task['task_id']
                prompt = task['prompt']
                entry_point = task['entry_point']
                test_code = task['test']
                
                # æ¯5ä¸ªä»»åŠ¡è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if idx % 5 == 0 or idx == total_tasks:
                    self.log(f"è¿›åº¦: {idx}/{total_tasks}")
                
                try:
                    # ç”Ÿæˆä»£ç 
                    with torch.no_grad():
                        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
                        
                        if torch.cuda.is_available():
                            inputs = {k: v.cuda() for k, v in inputs.items()}
                        
                        generated_ids = model.generate(
                            **inputs,
                            max_new_tokens=self.config.get("max_tokens", 512),
                            temperature=self.config.get("temperature", 0.7),
                            top_p=self.config.get("top_p", 0.9),
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            num_beams=1
                        )
                    
                    # è§£ç ä»£ç 
                    generated_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]
                    generated_code = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    
                    # æå–å‡½æ•°ä»£ç 
                    function_code = self.extract_function_code(generated_code, entry_point)
                    
                    # è¿è¡Œæµ‹è¯•
                    full_code = prompt + "\n" + function_code + "\n" + test_code
                    test_result = self.run_code_test(full_code, entry_point)
                    
                    if test_result["passed"]:
                        passed_tasks += 1
                    else:
                        failed_tasks.append(task_id)
                        
                    detailed_results.append({
                        "task_id": task_id,
                        "passed": test_result["passed"],
                        "error": test_result.get("error", "")
                    })
                    
                except Exception as e:
                    failed_tasks.append(task_id)
                    detailed_results.append({
                        "task_id": task_id,
                        "passed": False,
                        "error": str(e)
                    })
                
                # æ¸…ç†å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # è®¡ç®—æœ€ç»ˆç»“æœ
            elapsed_time = time.time() - start_time
            pass_rate = (passed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            result = {
                "model_name": model_name,
                "model_path": model_path,
                "total_tasks": total_tasks,
                "passed_tasks": passed_tasks,
                "failed_tasks_count": len(failed_tasks),
                "pass_rate": pass_rate,
                "elapsed_time": elapsed_time,
                "avg_time_per_task": elapsed_time / total_tasks if total_tasks > 0 else 0,
                "failed_task_ids": failed_tasks[:20],
                "detailed_results": detailed_results[:20],
                "evaluation_time": datetime.now().isoformat()
            }
            
            self.log(f"{model_name}è¯„ä¼°å®Œæˆ: {passed_tasks}/{total_tasks} é€šè¿‡ ({pass_rate:.2f}%)")
            
            # æ¸…ç†æ¨¡å‹
            del model, tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            return result
            
        except Exception as e:
            self.log(f"è¯„ä¼°{model_name}æ—¶å‡ºé”™: {str(e)}")
            self.log(traceback.format_exc())
            return None
    
    def extract_function_code(self, generated_text, entry_point):
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å‡½æ•°ä»£ç """
        text = generated_text.strip()
        
        # æ–¹å¼1: æ­£åˆ™åŒ¹é…
        pattern = rf'def\s+{re.escape(entry_point)}\s*\([^)]*\)\s*:.*?(?=\n\ndef\s+|\nclass\s+|$)'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(0).strip()
        
        # æ–¹å¼2: åŸºäºç¼©è¿›
        if f"def {entry_point}" in text:
            lines = text.split('\n')
            start_idx = -1
            for i, line in enumerate(lines):
                if f"def {entry_point}" in line:
                    start_idx = i
                    break
            
            if start_idx >= 0:
                result = [lines[start_idx]]
                base_indent = len(lines[start_idx]) - len(lines[start_idx].lstrip())
                
                for i in range(start_idx + 1, len(lines)):
                    line = lines[i]
                    if line.strip() and not line.startswith(' ' * (base_indent + 1)) and line.strip():
                        break
                    result.append(line)
                
                return '\n'.join(result)
        
        # æ–¹å¼3: è¿”å›æ•´ä¸ªæ–‡æœ¬
        return text
    
    def run_code_test(self, full_code, entry_point):
        """è¿è¡Œä»£ç æµ‹è¯•"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(full_code)
                temp_file = f.name
            
            # æ‰§è¡Œä»£ç 
            result = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                text=True,
                timeout=5,
                env={**os.environ, 'PYTHONPATH': ''}
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file)
            
            if result.returncode == 0:
                return {"passed": True}
            else:
                error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥é”™è¯¯"
                return {"passed": False, "error": error_msg}
                
        except subprocess.TimeoutExpired:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
            return {"passed": False, "error": "æ‰§è¡Œè¶…æ—¶"}
        except Exception as e:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
            return {"passed": False, "error": str(e)}
    
    def compare_results(self, original_result, finetuned_result):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ"""
        orig_rate = original_result["pass_rate"]
        fine_rate = finetuned_result["pass_rate"]
        improvement = fine_rate - orig_rate
        
        orig_passed = set(original_result.get("failed_task_ids", []))
        fine_passed = set(finetuned_result.get("failed_task_ids", []))
        
        newly_passed = list(orig_passed - fine_passed)  # åŸæ¥å¤±è´¥ï¼Œç°åœ¨é€šè¿‡
        newly_failed = list(fine_passed - orig_passed)  # åŸæ¥é€šè¿‡ï¼Œç°åœ¨å¤±è´¥
        
        return {
            "improvement": improvement,
            "original_pass_rate": orig_rate,
            "finetuned_pass_rate": fine_rate,
            "newly_passed_tasks": newly_passed[:10],
            "newly_failed_tasks": newly_failed[:10],
            "original_total_tasks": original_result["total_tasks"],
            "finetuned_total_tasks": finetuned_result["total_tasks"],
            "original_passed": original_result["passed_tasks"],
            "finetuned_passed": finetuned_result["passed_tasks"]
        }

def start_evaluation_interface(config_data):
    """å¼€å§‹è¯„ä¼°ç•Œé¢å‡½æ•°"""
    global is_evaluating, evaluation_thread
    
    if is_evaluating:
        return " è¯„ä¼°å·²ç»åœ¨è¿›è¡Œä¸­...", False
    
    # æ›´æ–°é…ç½®
    config = DEFAULT_CONFIG.copy()
    config.update(config_data)
    
    # æ£€æŸ¥å¿…è¦å‚æ•°
    required_fields = ["model_path", "finetuned_model_path", "human_eval_path"]
    for field in required_fields:
        if not config.get(field):
            return f" è¯·å¡«å†™{field}", False
    
    # æ£€æŸ¥è·¯å¾„
    for path_field in ["model_path", "finetuned_model_path", "human_eval_path"]:
        path = config[path_field]
        if not os.path.exists(path):
            return f" è·¯å¾„ä¸å­˜åœ¨: {path}", False
    
    # å¼€å§‹è¯„ä¼°çº¿ç¨‹
    evaluation_thread = EvaluationThread(config, log)
    is_evaluating = True
    evaluation_thread.start()
    
    start_msg = f"""
 å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...
åŸå§‹æ¨¡å‹: {config['model_path']}
å¾®è°ƒæ¨¡å‹: {config['finetuned_model_path']}
æ•°æ®é›†: {config['human_eval_path']}
æœ€å¤§ä»»åŠ¡æ•°: {config['max_tasks']}
    
è¯„ä¼°æ—¥å¿—å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º...
    """
    
    log(start_msg)
    return " è¯„ä¼°å·²å¼€å§‹", True

def get_comparison_results():
    """è·å–å¯¹æ¯”ç»“æœ"""
    global comparison_results
    
    if not comparison_results:
        return "æš‚æ— è¯„ä¼°ç»“æœ"
    
    result_text = f"""
# æ¨¡å‹å¯¹æ¯”è¯„ä¼°ç»“æœ

## æ€»ä½“è¡¨ç°
- **åŸå§‹æ¨¡å‹é€šè¿‡ç‡**: {comparison_results['original_pass_rate']:.2f}%
- **å¾®è°ƒæ¨¡å‹é€šè¿‡ç‡**: {comparison_results['finetuned_pass_rate']:.2f}%
- **æå‡æ•ˆæœ**: {comparison_results['improvement']:+.2f}%

## è¯¦ç»†æ•°æ®
- åŸå§‹æ¨¡å‹: {comparison_results['original_passed']}/{comparison_results['original_total_tasks']} é€šè¿‡
- å¾®è°ƒæ¨¡å‹: {comparison_results['finetuned_passed']}/{comparison_results['finetuned_total_tasks']} é€šè¿‡

## æ”¹è¿›åˆ†æ
"""
    
    if comparison_results['newly_passed_tasks']:
        result_text += f"- **æ–°é€šè¿‡çš„ä»»åŠ¡**: {len(comparison_results['newly_passed_tasks'])} ä¸ª\n"
        if comparison_results['newly_passed_tasks']:
            result_text += f"  ç¤ºä¾‹: {', '.join(comparison_results['newly_passed_tasks'][:5])}\n"
    
    if comparison_results['newly_failed_tasks']:
        result_text += f"- **æ–°å¤±è´¥çš„ä»»åŠ¡**: {len(comparison_results['newly_failed_tasks'])} ä¸ª\n"
        if comparison_results['newly_failed_tasks']:
            result_text += f"  ç¤ºä¾‹: {', '.join(comparison_results['newly_failed_tasks'][:5])}\n"
    
    if comparison_results['improvement'] > 0:
        result_text += "\n **å¾®è°ƒæ•ˆæœ: æå‡æ˜æ˜¾**"
    elif comparison_results['improvement'] == 0:
        result_text += "\n **å¾®è°ƒæ•ˆæœ: æ— æ˜æ˜¾å˜åŒ–**"
    else:
        result_text += "\n **å¾®è°ƒæ•ˆæœ: æ€§èƒ½ä¸‹é™**"
    
    return result_text

is_evaluating = False
evaluation_thread = None
